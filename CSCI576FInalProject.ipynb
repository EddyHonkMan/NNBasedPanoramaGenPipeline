{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CSCI576 Final Project\n",
    "\n",
    "**Rui Zhu** \n",
    "\n",
    "**Yuyang Huang**\n",
    "\n",
    "**Zixun Lu**\n",
    "\n",
    "**Junyu Yan**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Requirements\n",
    "\n",
    "**Python 3.10**\n",
    "\n",
    "**opencv-python==4.6.0.66**\n",
    "\n",
    "**carvekit https://anodev.ru/image-background-remove-tool/**\n",
    "\n",
    "**Please put the input video into the \"videos\" folder of your current path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shelve\n",
    "\n",
    "# Generate foreground labels\n",
    "import torch\n",
    "from PIL import Image\n",
    "from carvekit.api.interface import Interface\n",
    "from carvekit.ml.wrap.fba_matting import FBAMatting\n",
    "from carvekit.ml.wrap.tracer_b7 import TracerUniversalB7\n",
    "from carvekit.ml.wrap.u2net import U2NET\n",
    "from carvekit.pipelines.postprocessing import MattingMethod\n",
    "from carvekit.pipelines.preprocessing import PreprocessingStub\n",
    "from carvekit.trimap.generator import TrimapGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Prepare global variables and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**If you want to use another test video instead of \"test3.mp4\", please change the FILENAME accordingly on Line 31**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_frames(path):\n",
    "    \"\"\"\n",
    "    return video frames\n",
    "    \"\"\"\n",
    "    cap = cv.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Open video failed!\")\n",
    "\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            break\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "\n",
    "# prepare global variables\n",
    "INPUT_VIDEO_DIR = 'videos'\n",
    "dir(INPUT_VIDEO_DIR)\n",
    "OUTPUT_PATH = 'out'\n",
    "dir(OUTPUT_PATH)\n",
    "\n",
    "FILENAME = 'test3.mp4'\n",
    "INPUT_VIDEO_PATH = os.path.join(INPUT_VIDEO_DIR, FILENAME)\n",
    "\n",
    "\n",
    "OUTPUT_FOREGROUND_VIDEO_NAME = FILENAME.split('.')[0] + '_foreground.mp4'\n",
    "OUTPUT_BACKGROUND_VIDEO_NAME = FILENAME.split('.')[0] + '_background.mp4'\n",
    "OUTPUT_PANORAMA_IMG_NAME = FILENAME.split('.')[0] + '_panorama.jpg'\n",
    "HOLE_FILLED_BACKGROUND_VIDEO_NAME = 'hole_filled_' + FILENAME.split('.')[0] + '_background.mp4'\n",
    "\n",
    "OUTPUT_FOREGROUND_VIDEO_PATH = os.path.join(OUTPUT_PATH, OUTPUT_FOREGROUND_VIDEO_NAME)\n",
    "OUTPUT_BACKGROUND_VIDEO_PATH = os.path.join(OUTPUT_PATH, OUTPUT_BACKGROUND_VIDEO_NAME)\n",
    "OUTPUT_PANORAMA_IMG_PATH = os.path.join(OUTPUT_PATH, OUTPUT_PANORAMA_IMG_NAME)\n",
    "HOLE_FILLED_BACKGROUND_VIDEO_PATH = os.path.join(OUTPUT_PATH, HOLE_FILLED_BACKGROUND_VIDEO_NAME)\n",
    "\n",
    "H_PERSIST_FILENAME = 'H_persist'\n",
    "FOREGROUND_LABELS_PERSISTENCE_FILENAME = '{}_foreground_labels.npy'.format(FILENAME.split('.')[0])\n",
    "\n",
    "\n",
    "cap = cv.VideoCapture(INPUT_VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Open video failed!\")\n",
    "\n",
    "fps = int(cap.get(cv.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frame = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 1: Separate background and foreground\n",
    "\n",
    "###  1.1 Use [neural networks](https://github.com/OPHoperHPO/image-background-remove-tool) to remove the background from each frame of the video\n",
    "> This neural networks will remove the background from the input image and outputs an image with only the foreground and alpha of 0 in the background part.\n",
    "### 1.2 Generate foreground labels in each frame\n",
    "    1. Put each frame of the original video into nn to get background removed image/frame.\n",
    "    2. Compare the foreground-only output frame with original frame pixel by pixel.\n",
    "    3. Label pixel as foreground if alpha value is not 0.\n",
    "\n",
    "**Since the background removal of the entire video consumes time, we cached all labels for development purposes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_foreground_labels(path, early_quit_frame_number=math.inf, early_quit_time=None):\n",
    "\n",
    "    # NN setting\n",
    "    _device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    seg_net = TracerUniversalB7(device=_device, batch_size=5, input_image_size=640, fp16=True)\n",
    "    fba = FBAMatting(device=_device, input_tensor_size=2048, batch_size=1)\n",
    "    trimap = TrimapGenerator(prob_threshold=231, kernel_size=30, erosion_iters=5)\n",
    "    preprocessing = PreprocessingStub()\n",
    "    postprocessing = MattingMethod(matting_module=fba, trimap_generator=trimap, device=_device)\n",
    "    interface = Interface(pre_pipe=preprocessing, post_pipe=postprocessing, seg_pipe=seg_net)\n",
    "\n",
    "\n",
    "    cap = cv.VideoCapture(path)\n",
    "    if not cap.isOpened:\n",
    "        raise IOError(\"Open video failed!\")\n",
    "\n",
    "    foreground_labels = []\n",
    "    startTime = time.time()\n",
    "    try:\n",
    "        for _ in tqdm(range(int(min(cap.get(cv.CAP_PROP_FRAME_COUNT), early_quit_frame_number)))):\n",
    "            if early_quit_time and time.time() - startTime > early_quit_time:\n",
    "                break\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                raise IOError(\"Read frame failed!\")\n",
    "\n",
    "            frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame, mode=\"RGB\")\n",
    "            resImg = interface([image])[0]\n",
    "            foreground = np.asarray(resImg)\n",
    "            foreground_label = set()\n",
    "            for i in range(len(foreground)):\n",
    "                for j in range(len(foreground[0])):\n",
    "                    if foreground[i][j][-1] != 0:\n",
    "                        foreground_label.add((i, j))\n",
    "            foreground_labels.append(foreground_label)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted!')\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "    return foreground_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foreground_labels = None\n",
    "\n",
    "# if there exists foreground label cache file, just load it. Otherwise, compute it\n",
    "if not os.path.exists(FOREGROUND_LABELS_PERSISTENCE_FILENAME):\n",
    "    print('Generate foreground labels...')\n",
    "    # compute foreground labels\n",
    "    foreground_labels = get_foreground_labels(INPUT_VIDEO_PATH)\n",
    "    # save foreground labels\n",
    "    np.save(FOREGROUND_LABELS_PERSISTENCE_FILENAME, [list(label) for label in foreground_labels])\n",
    "    print('Generate labels done.')\n",
    "else:\n",
    "    print('Load persisted file...')\n",
    "    # If there is stored foreground label file, load it locally.\n",
    "    foreground_labels = np.load(FOREGROUND_LABELS_PERSISTENCE_FILENAME, allow_pickle=True)\n",
    "    # convert foreground_labels to list of set\n",
    "    foreground_labels = [set(_) for _ in foreground_labels]\n",
    "    print('Load labels done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 2 Generate foreground video and background video without holes\n",
    "\n",
    "\n",
    "## 2.1 generate foreground video and background video with holes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_foreground_background_videos(foreground_labels):\n",
    "    if os.path.exists(OUTPUT_FOREGROUND_VIDEO_PATH) and os.path.exists(OUTPUT_BACKGROUND_VIDEO_PATH):\n",
    "        return\n",
    "    if not os.path.exists('out'):\n",
    "        os.makedirs('out')\n",
    "\n",
    "    cap = cv.VideoCapture(INPUT_VIDEO_PATH)\n",
    "    if not cap.isOpened:\n",
    "        raise IOError(\"Open video failed!\")\n",
    "\n",
    "    foreground_out = cv.VideoWriter(OUTPUT_FOREGROUND_VIDEO_PATH, cv.VideoWriter_fourcc(*'XVID'), int(cap.get(cv.CAP_PROP_FPS)),\n",
    "                                (int(cap.get(cv.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))\n",
    "    background_out = cv.VideoWriter(OUTPUT_BACKGROUND_VIDEO_PATH, cv.VideoWriter_fourcc(*'XVID'), int(cap.get(cv.CAP_PROP_FPS)),\n",
    "                                (int(cap.get(cv.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))\n",
    "    if not foreground_out.isOpened() or not background_out.isOpened():\n",
    "        raise IOError(\"Init videoWriter failed!\")\n",
    "\n",
    "    try:\n",
    "        for i in tqdm(range(len(foreground_labels))):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                raise IOError(\"Read frame failed!\")\n",
    "                \n",
    "            label = foreground_labels[i]\n",
    "            foreground = np.zeros_like(frame)\n",
    "            foreground.fill(255)\n",
    "            for (j, k) in label:\n",
    "                foreground[j][k] = frame[j][k]\n",
    "                frame[j][k] = [255,255,255]\n",
    "\n",
    "            foreground_out.write(foreground.copy())\n",
    "            background_out.write(frame.copy())\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted!')    \n",
    "\n",
    "    cap.release()\n",
    "    foreground_out.release()\n",
    "    background_out.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate foreground video and background video with holes, if they don't exist\n",
    "generate_foreground_background_videos(foreground_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 fill background holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute Homography matrix from src to dst\n",
    "def get_H_matrix(src, dst):\n",
    "    # minimum number of matches we want find between these two images\n",
    "    min_match_threshold = 10\n",
    "\n",
    "    # initiate feature detector, currently use SIFT, may try orb later\n",
    "    sift = cv.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(src,None)\n",
    "    kp2, des2 = sift.detectAndCompute(dst,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "    flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good)>min_match_threshold:\n",
    "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        H, _ = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\n",
    "    else:\n",
    "        raise ValueError( \"Not enough matches are found - {}/{}\".format(len(good), min_match_threshold) )\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill i_th frame's hole using adjacent frames per gap\n",
    "def fill_hole(background_frames, foreground_labels, i, gap = 1):\n",
    "    \"\"\"\n",
    "    :param frames: list of background frames with holes\n",
    "    :param foreground_labels: list of foreground labels for each frame\n",
    "    :param i: index of the frame we want fill\n",
    "    :param gap: is the distance unit between target frame and current frame when filling hole\n",
    "    :return: hole_filled frame\n",
    "    \"\"\"\n",
    "    assert len(background_frames) == len(foreground_labels)\n",
    "    assert i in range(len(background_frames))\n",
    "    assert gap >= 1\n",
    "\n",
    "    # should not change original set, otherwise when we are using this frame to fill other frames' holes, result would be a mess!\n",
    "    cur_label = foreground_labels[i].copy()\n",
    "    res_frame = background_frames[i]\n",
    "    left = 1\n",
    "    right = 1\n",
    "    last_direction = False\n",
    "\n",
    "    # while there are still pixels not filled in current frame\n",
    "    while len(cur_label) > 0:\n",
    "        # we iterate first to right, then to left, then back to right, then back to left... \n",
    "        # each time we try to grab some pixels from target frame which are not foreground pixels there\n",
    "        target_index = i + right * gap if not last_direction else i - left * gap\n",
    "\n",
    "        if 0 <= target_index < len(background_frames):\n",
    "            target_frame_foreground_label = foreground_labels[target_index]\n",
    "            srcs = [list(t) for t in cur_label]\n",
    "\n",
    "            # point x = j, y = i, so reverse below in a[::-1]\n",
    "            # perspectiveTransform all pixels in current foreground to target index frame\n",
    "            # perspectiveTransform needs 3 dimensions, so we manually wrap a dimension to make below 1 * N * 2\n",
    "            srcpts = np.array([[a[::-1] for a in srcs]]).astype(np.float32)\n",
    "            H = get_H_matrix(background_frames[i], background_frames[target_index])\n",
    "            dstpts = np.rint(cv.perspectiveTransform(srcpts, H)[0]).astype(int)\n",
    "            # each pixel in current frame's foreground is perspective transformed to target frame\n",
    "            # for each of these pixels, we check if its corresponding pixel is not in target frame's foreground label\n",
    "            # then check its corresponding pixel is in range\n",
    "            # if all satisfied, fill back\n",
    "            for (x, y), (corresponding_x, corresponding_y) in zip(np.rint(srcpts[0]).astype(int), dstpts):\n",
    "                if (corresponding_y, corresponding_x) not in target_frame_foreground_label and 0 <= corresponding_x < width and 0 <= corresponding_y < height:\n",
    "                    cur_label.remove((y, x))\n",
    "                    res_frame[y][x] = background_frames[target_index][corresponding_y][corresponding_x]\n",
    "\n",
    "            # if this time to right\n",
    "            if not last_direction:\n",
    "                right += 1\n",
    "            else:\n",
    "                left += 1\n",
    "\n",
    "        last_direction = not last_direction\n",
    "\n",
    "    return res_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Generate videos with foreground objects removed and hole filled (for application 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill all background frames' holes and save them to image and video\n",
    "def generate_hole_filled_background_video():\n",
    "    if os.path.exists(HOLE_FILLED_BACKGROUND_VIDEO_PATH):\n",
    "        return\n",
    "    background_frames = read_frames(OUTPUT_BACKGROUND_VIDEO_PATH)\n",
    "    out_video = cv.VideoWriter(HOLE_FILLED_BACKGROUND_VIDEO_PATH, cv.VideoWriter_fourcc(*'XVID'), fps, (width, height))\n",
    "    assert out_video.isOpened()\n",
    "    if not os.path.exists('hole_filled_background'):\n",
    "        os.mkdir('hole_filled_background')\n",
    "        \n",
    "    try:\n",
    "        for i in tqdm(range(len(background_frames))):\n",
    "            res = fill_hole(background_frames, foreground_labels, i, 6)\n",
    "            cv.imwrite(os.path.join('hole_filled_background','{}_background_{}.jpg'.format(FILENAME.split('.')[0], i)), res)\n",
    "            # out_video.write(res)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted!\")\n",
    "\n",
    "    out_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_hole_filled_background_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 3: Generate panorama for the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# get lower resolution image for faster H matrix computation\n",
    "def scale_image(img, scale_factor=1):\n",
    "    return cv.resize(img, [int(round(img.shape[1] * scale_factor)), int(round(img.shape[0] * scale_factor))], interpolation=cv.INTER_LINEAR_EXACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stitch src to dst, with dst padded, return padded result with shifted_homography matrix, for possible future mapping into the panorama\n",
    "def stitch(dst, src, H):\n",
    "    assert H.shape == (3,3)\n",
    "\n",
    "    # ensure a legal homography\n",
    "    H = H / H[2, 2]  \n",
    "    src_h, src_w = src.shape[:2]\n",
    "    lin_homg_pts = np.array([\n",
    "        [0, src_w, src_w, 0],\n",
    "        [0, 0, src_h, src_h],\n",
    "        [1, 1, 1, 1]])\n",
    "        # transform points\n",
    "    transf_lin_homg_pts = H.dot(lin_homg_pts)\n",
    "    transf_lin_homg_pts /= transf_lin_homg_pts[2, :]\n",
    "\n",
    "    # find min and max points\n",
    "    min_x = np.floor(np.min(transf_lin_homg_pts[0])).astype(int)\n",
    "    min_y = np.floor(np.min(transf_lin_homg_pts[1])).astype(int)\n",
    "    max_x = np.ceil(np.max(transf_lin_homg_pts[0])).astype(int)\n",
    "    max_y = np.ceil(np.max(transf_lin_homg_pts[1])).astype(int)\n",
    "\n",
    "    # add translation to the transformation matrix to shift to positive values\n",
    "    anchor_x, anchor_y = 0, 0\n",
    "    transl_transf = np.eye(3, 3)\n",
    "    if min_x < 0:\n",
    "        anchor_x = -min_x\n",
    "        transl_transf[0, 2] += anchor_x\n",
    "    if min_y < 0:\n",
    "        anchor_y = -min_y\n",
    "        transl_transf[1, 2] += anchor_y\n",
    "    shifted_transf = transl_transf.dot(H)\n",
    "    shifted_transf /= shifted_transf[2, 2]\n",
    "\n",
    "    dst_h, dst_w = dst.shape[:2]\n",
    "    padding = [anchor_y, max(max_y, dst_h) - dst_h,\n",
    "                  anchor_x, max(max_x, dst_w) - dst_w]\n",
    "    \n",
    "    stitched_image = cv.warpPerspective(\n",
    "        src, shifted_transf, (dst_w + padding[2] + padding[3], dst_h + padding[0] + padding[1]),\n",
    "        flags=cv.INTER_LINEAR, borderMode=cv.BORDER_CONSTANT, borderValue=0)\n",
    "    \n",
    "    # make the final effect as, the new image(i.e. the src image) is padding the external hole of the padded_dst\n",
    "    for i in range(0, dst_h):\n",
    "        for j in range(0, dst_w):\n",
    "            if any(dst[i][j]):\n",
    "                stitched_image[i + padding[0]][j + padding[2]] = dst[i][j]\n",
    "\n",
    "    return stitched_image, shifted_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a lot lot lot of tests regarding how step size and the way that homography is generated in each iteration would affect final panorama quality,\n",
    "# we finally chose to use new hole_filled_frames to compute homography matrix with so-far generated panorama and then use this H to stitch and create new panorama for next iteration\n",
    "# we grow this panorama follow right, left, right, left pattern for better result\n",
    "def generate_panorama_using_hole_filled_frames(right_step =30, left_step=30, anchor_index=-1, intermediate_panoramas=None):\n",
    "    hole_filled_frames = read_frames(HOLE_FILLED_BACKGROUND_VIDEO_PATH)\n",
    "    assert -1 <= anchor_index < len(hole_filled_frames)\n",
    "    assert right_step > 0 and left_step > 0\n",
    "\n",
    "    anchor_index = len(hole_filled_frames) // 2 if anchor_index == -1 else anchor_index\n",
    "    shifted_H_persist = {}\n",
    "\n",
    "    try:\n",
    "        panorama = hole_filled_frames[anchor_index].copy()\n",
    "        left = anchor_index - left_step\n",
    "        right = anchor_index + right_step\n",
    "        last_direction = False\n",
    "        count = 1\n",
    "\n",
    "        while left >= 0 or right < len(hole_filled_frames):\n",
    "            print('iteration {}, time: {}'.format(count, time.ctime()))\n",
    "            count += 1\n",
    "            # the panorama grows first to right then to left\n",
    "            if not last_direction:\n",
    "                if right < len(hole_filled_frames):\n",
    "                    H = get_H_matrix(scale_image(hole_filled_frames[right], scale_factor=1), scale_image(panorama, scale_factor=1))\n",
    "                    panorama, shifted_H = stitch(panorama, hole_filled_frames[right], H)\n",
    "                    shifted_H_persist[(right, anchor_index)] = shifted_H\n",
    "                    if intermediate_panoramas is not None:\n",
    "                        intermediate_panoramas.append(panorama)\n",
    "                    right += right_step\n",
    "            else:\n",
    "                if left >= 0:\n",
    "                    H = get_H_matrix(scale_image(hole_filled_frames[left], scale_factor=1), scale_image(panorama, scale_factor=1))\n",
    "                    panorama, shifted_H = stitch(panorama, hole_filled_frames[left], H)\n",
    "                    shifted_H_persist[(left, anchor_index)] = shifted_H\n",
    "                    if intermediate_panoramas is not None:\n",
    "                        intermediate_panoramas.append(panorama)\n",
    "                    left -= left_step\n",
    "            last_direction = not last_direction\n",
    "        return panorama, shifted_H_persist\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediate_panoramas = []\n",
    "panorama, shifted_H_persist = generate_panorama_using_hole_filled_frames(right_step=48, left_step=48, anchor_index=-1, intermediate_panoramas=intermediate_panoramas)\n",
    "cv.imwrite(OUTPUT_PANORAMA_IMG_PATH, panorama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can view the intermediate panorama results here\n",
    "for p in intermediate_panoramas:\n",
    "    cv.imshow('intermediate panorama', p)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "    # this line is for macos compatibility\n",
    "    cv.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 4: Create Application 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interpolate_foreground_on_panorama(panorama_img, foreground_labels_set):\n",
    "    for i in range(4, len(panorama_img) - 4):\n",
    "        for j in range(4, len(panorama_img[0]) - 4):\n",
    "            if (i, j) in foreground_labels_set:\n",
    "                continue\n",
    "            fore_count = 0\n",
    "            cur_pix = np.array([0, 0, 0])\n",
    "            for m in range(i - 2, i + 3):\n",
    "                for n in range(j - 2, j + 3):\n",
    "                    if m == i and n == j:\n",
    "                        continue\n",
    "                    if (m, n) in foreground_labels_set:\n",
    "                        fore_count += 1\n",
    "                        cur_pix = np.add(cur_pix, panorama_img[m][n])\n",
    "            if fore_count >= 5:\n",
    "                panorama_img[i][j] = cur_pix / fore_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Application 1\n",
    "def fill_in_foreground_and_generate_img(n):\n",
    "\n",
    "    cap = cv.VideoCapture(INPUT_VIDEO_PATH)\n",
    "    if not cap.isOpened:\n",
    "        raise IOError(\"Open video failed!\")\n",
    "\n",
    "    panorama = cv.imread(OUTPUT_PANORAMA_IMG_PATH)\n",
    "    panorama_foreground_set = set()\n",
    "    panorama_height = len(panorama)\n",
    "    panorama_width = len(panorama[0])\n",
    "    try:\n",
    "        for i in tqdm(range(1, int(cap.get(cv.CAP_PROP_FRAME_COUNT)))):\n",
    "            ret, prev = cap.read()\n",
    "            if not ret:\n",
    "                raise IOError(\"Read frame failed!\")\n",
    "\n",
    "            if i % n == 0:\n",
    "                H = get_H_matrix(prev, panorama)\n",
    "                cur_label = foreground_labels[i].copy()\n",
    "                srcs = [list(t) for t in cur_label]\n",
    "                srcpts = np.array([[a[::-1] for a in srcs]]).astype(np.float32)\n",
    "                dstpts = np.rint(cv.perspectiveTransform(srcpts, H)[0]).astype(int)\n",
    "                for (x, y), (corresponding_x, corresponding_y) in zip(np.rint(srcpts[0]).astype(int), dstpts):\n",
    "                    if 0 <= corresponding_x < panorama_width and 0 <= corresponding_y < panorama_height:\n",
    "                        panorama[corresponding_y][corresponding_x] = prev[y][x]\n",
    "                        panorama_foreground_set.add((corresponding_y, corresponding_x))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted!')\n",
    "\n",
    "    interpolate_foreground_on_panorama(panorama, panorama_foreground_set)\n",
    "        # cv.imshow(str(frame_count), prev)\n",
    "        # cv.waitKey(0)\n",
    "        # cv.destroyAllWindows()\n",
    "        # cv.waitKey(1)\n",
    "\n",
    "    cap.release()\n",
    "    cv.imshow('panorama', panorama)\n",
    "    cv.imwrite(os.path.join(OUTPUT_PATH, \"{}_panorama_output1.jpg\".format(FILENAME.split('.')[0])), panorama)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "    cv.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in foreground every n frames\n",
    "fill_in_foreground_and_generate_img(56)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 5: Create Application 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Application 2 pre video (Firstly generate all images)\n",
    "def fill_in_foreground_and_generate_all_imgs(output_folder, start, end):\n",
    "    cap = cv.VideoCapture(INPUT_VIDEO_PATH)\n",
    "\n",
    "    if not cap.isOpened:\n",
    "        raise IOError(\"Open video failed!\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    panorama = cv.imread(OUTPUT_PANORAMA_IMG_PATH)\n",
    "    panorama_height = len(panorama)\n",
    "    panorama_width = len(panorama[0])\n",
    "    panorama_foreground_set = set()\n",
    "    try:\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if start >= int(cap.get(cv.CAP_PROP_FRAME_COUNT)):\n",
    "            return\n",
    "        if end > int(cap.get(cv.CAP_PROP_FRAME_COUNT)):\n",
    "            end = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "        for i in tqdm(range(0, int(cap.get(cv.CAP_PROP_FRAME_COUNT)))):\n",
    "            ret, prev = cap.read()\n",
    "            if not start <= i < end:\n",
    "                continue\n",
    "            if not ret:\n",
    "                raise IOError(\"Read frame failed!\")\n",
    "\n",
    "            panorama_foreground_set.clear()\n",
    "            cur_panorama = panorama.copy()\n",
    "            H = get_H_matrix(prev, cur_panorama)\n",
    "            cur_label = foreground_labels[i].copy()\n",
    "            srcs = [list(t) for t in cur_label]\n",
    "            if len(srcs) != 0:\n",
    "                srcpts = np.array([[a[::-1] for a in srcs]]).astype(np.float32)\n",
    "                dstpts = np.rint(cv.perspectiveTransform(srcpts, H)[0]).astype(int)\n",
    "                for (x, y), (corresponding_x, corresponding_y) in zip(np.rint(srcpts[0]).astype(int), dstpts):\n",
    "                    if 0 <= corresponding_x < panorama_width and 0 <= corresponding_y < panorama_height:\n",
    "                        cur_panorama[corresponding_y][corresponding_x] = prev[y][x]\n",
    "                        panorama_foreground_set.add((corresponding_y, corresponding_x))\n",
    "\n",
    "                interpolate_foreground_on_panorama(cur_panorama, panorama_foreground_set)\n",
    "            cv.imwrite(f\"{output_folder}/{FILENAME.split('.')[0]}_panorama_with_foreground_{i}.jpg\", cur_panorama)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted!')\n",
    "\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_in_foreground_and_generate_all_imgs('output3_best', 0, 595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createAppication2Video(panoBackground: np.ndarray, frames: np.ndarray,\n",
    "    start: tuple[int, int], end: tuple[int, int], dimension=(1920, 1080)) -> list[np.ndarray]:\n",
    "    start = updatePosition(*start, *dimension, panoBackground.shape[1], panoBackground.shape[0])\n",
    "    end = updatePosition(*end, *dimension, panoBackground.shape[1], panoBackground.shape[0])\n",
    "\n",
    "    # print(\"panoBackground.shape[1]: \", panoBackground.shape[1])\n",
    "    # print(\"panoBackground.shape[0]: \", panoBackground.shape[0])\n",
    "\n",
    "    dx = (end[0] - start[0]) / frames.shape[0]\n",
    "    dy = (end[1] - start[1]) / frames.shape[1]\n",
    "    halfWidth = int(0.5 * dimension[0])\n",
    "    halfHeight = int(0.5 * dimension[1])\n",
    "\n",
    "    new_frames = []\n",
    "    camera_center: list[float] = [start[0], start[1]]\n",
    "\n",
    "    for i in range(frames.shape[0]):\n",
    "        frame = frames[i]\n",
    "        lx, rx = int(camera_center[0] - halfWidth), int(camera_center[0] + halfWidth)\n",
    "        ly, ry = int(camera_center[1] - halfHeight), int(camera_center[1] + halfHeight)\n",
    "\n",
    "        # print(\"lx, rx: \", lx, rx)\n",
    "        # print(\"ly, ry: \", ly, ry)\n",
    "\n",
    "        new_frames.append(frame[ly:ry, lx:rx])\n",
    "        camera_center[0] += dx\n",
    "        camera_center[1] += dy\n",
    "    return new_frames\n",
    "\n",
    "def updatePosition(x: int, y: int, cameraWidth: int,\n",
    "                           cameraHeight: int, backgroundWidth: int, backgroundHeight: int) -> tuple[int, int]:\n",
    "\n",
    "    # print(\"width: \", cameraWidth // 2, min(backgroundWidth - cameraWidth // 2, x))\n",
    "    # print(\"height: \", cameraHeight // 2, min(backgroundHeight - cameraHeight // 2, y))\n",
    "\n",
    "    return (\n",
    "        max(cameraWidth // 2, min(backgroundWidth - cameraWidth // 2, x)),\n",
    "        max(cameraHeight // 2, min(backgroundHeight - cameraHeight // 2, y)),\n",
    "    )\n",
    "\n",
    "def saveApplication2Output(filename: str, frames: list[np.ndarray] | np.ndarray,\n",
    "          wid: int, hei: int) -> None:\n",
    "    file = cv.VideoWriter(f'out/{filename}.mp4', cv.VideoWriter_fourcc(*'mp4v'), 30, (wid, hei))\n",
    "    for frame in frames:\n",
    "        file.write(frame)\n",
    "    file.release()\n",
    "\n",
    "resolution = (1920, 1080)\n",
    "\n",
    "# panoVideo = loadVideo('test3_panorama_video.mp4')\n",
    "\n",
    "panoVideo = []\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(0, 595)):\n",
    "        cur_img_name = f\"output3_best/{FILENAME.split('.')[0]}_panorama_with_foreground_{i}.jpg\"\n",
    "\n",
    "        if os.path.isfile(cur_img_name):\n",
    "            curFrame = cv.imread(cur_img_name)\n",
    "            curFrame = cv.resize(curFrame, (int(curFrame.shape[1] * 0.44), int(curFrame.shape[0] * 0.44)))\n",
    "            panoVideo.append(curFrame)\n",
    "            # panorama_video_out.write(cv.imread(cur_img_name))\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted!')\n",
    "\n",
    "pano = cv.imread(OUTPUT_PANORAMA_IMG_PATH)\n",
    "out2 = createAppication2Video(pano, np.array(panoVideo), (600, 10), (3120, 10), resolution)\n",
    "saveApplication2Output('test3Application2output', out2, 1920, 1080)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "efa7729f80b0da6b29bb477aad01184eefb8234e0da7f2495a8ab64a024c45a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
